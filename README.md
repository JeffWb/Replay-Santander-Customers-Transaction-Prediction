# Replay-Santander-Customers-Transaction-Prediction
这是一篇关于kaggle比赛的复盘
Santander Customer Transaction Prediction
一 比赛前期
1.	比赛简介
预测客户是否会做出交易，标签为1表示会做出交易，为0表示不会做出交易。训练和测试数据各有200000个，包括200个匿名特征，最终的评价指标为auc得分。

2.	前期数据探索
（1）	数据清洗：判断是否有缺失值  df.isnull();df.info()。该问题中不含有缺失值
（2）	正负样本不平衡问题，正:负 ≈ 7:1，但是由于评价指标为auc，所以样本数据不平衡问题的影响较小。不需要做处理
（3）	训练数据集和测试数据集具有极为相似的分布。具体的对训练集合测试集每一个特征进行sns.distplot，观察其分布，发现每一个特征在训练集合测试集上的分布非常相似。

3.	特征工程
	因为特征都是匿名，所以前期的特征工程没有做，只是简单地对连续值进行了离散化，主要因为离散化的数据鲁棒性更强一些。具体的train[“var”] = pd.cut(train[“var”], bins, labels = [])

4.	模型选择
先从简单的模型开始，一次尝试了LR，Decision Tree，Random Forest，Lightgbm
（1）	LR：使用GridSearchCV搜索最优参数C，最后取模型的预测概率，最终得分0.860
（2） Decision Tree：依旧使用GridSearchCV进行调参，主要参数有特征的划分标准（criterion），树的深度（max_depth），最小划分样本数（min_samples_split）划分时考虑的最大特征数（max_features）,最终得分较低0.819
（3） Random Forest：使用GridSearchCV进行调参，主要参数有决策树个数（n_estimators），特征划分标准(criterion),树的深度（max_depth），划分时考虑的最大特征数（max_features），最小划分样本数（min_samples_split）最终得分为0.854
（4） lightgbm：这里没有选用xgboost，主要是数据较大，lightgbm的运行速度更快。调参方式为手动调节，先将learning_rate设置的大一些，以加快训练，每次固定其他参数，对某一参数进行调节，主要调节参数有：max_depth,num_leaves,learning_rate,bagging_freq,bagging_fraction,feature_fraction,最终得		分为0.900~0.901

二 比赛中期
	此时比赛进入了一个瓶颈期，尝试各种各样的方法，比如数据标准化，PCA特征，选择构造新特征（每一个样本的所有特征值的均值，方差，标准差等等），使用NN，都没有办法突破0.901的分数，几乎所有人都卡在0.901，只有少数人突破了0.901，并且他们将突破0.901的方法称为magic，于是开始广泛的浏览kernel和							discussion，寻找magic。 

三 比赛后期
Magic.1
	所有200个特征之间都是相互独立的：主要来自一篇kernel的分析，主要工作是将遍历所有特征，将该特征的取值全部打乱，再次使用lightgbm计算，得到的分数			依然能够到达很高的0.899~0.900。按照常理，最终分数肯定会比0.901低很多，但是并没有。于是有以下两个结论：
	A．	特征之间几乎没有相互作用。因为如果有相互作用，那么打乱数据会使得相关系数发生变化，从而影响结果。
	B．	可以尝试对数据进行count编码，创建新特征。因为在打乱数据前后，有一个东西没有发生改变，那就是特征的值在该特征中出现的次数。

Magic.2（来自于某个kernel）
	在test数据中存在合成数据，这部分数据是不参与得分计算的。之所以有这样的假设是因为：
	A．	训练数据集和测试数据集样本数相当。这点只是猜测，因为一般的比赛中，测试集的数目要小于训练集的数目，而这个是刚好相等，比较可疑。
	B．	由前期的数据探索，每一个特征在训练集合测试集上的分布都非常相似
 
 
猜测测试集中可能有一部分数据是由训练集合成，如果真的有这样的样本，那么合成测试样本的方法，应该是：对于合成样本的某一特征取值为从训练集相应的特征中随机选取的值，从而构成合成样本。这样的话则可以通过测试集的唯一值找出合成数据。找出的合成数据刚好100000个。
 

根据以上两个Magic进行特征工程，
1.	首先将test中的合成数据去除，然后将train和test合并进行count encoding创建新特征，再将train，test分开，最终的数据集train（200000，402），test（100000,402）。训练lightgbm，得分为0.908.
2.	虽然使用上面的方法突破了0.901，得分达到了0.909，但是依然有个条件没有用上，那就是任意特征之间相互独立，为了充分使用这个条件，每次只训练两个特征，一个是原始特征，一个是对应的count特征，这样可以防止lightgbm学习特征之间的伪相关性。最终成绩得分为0.913。


四 总结
1.	这个比赛和其他比赛不同，全部为匿名特征，而且测试集中含有合成数据，大部分特征工程也没有很好的提高，比如离散化，标准化，特征选择，够早的常见的特征等。
2.	深入学习了lightgbm的使用，以及调参过程。可以手动调节，可以使用GridSearchCV，也可以使用BayesSearchCV
3.	虽然最后取得了不错的分数，但是仍然有提高的空间，比如使用模型融合stack。使用5折交叉验证，每两个特征训练一个模型，一共200个模型，对于每一个fold，都有200个模型对原始test和val_data的预测结果，于是每一个fold都可以得到一个[num_samples,200]矩阵，对于train将200个矩阵相加即可，对于test将200矩阵相加在取平均，最后使用LR进行下一层训练得到最终结果。
